{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+----------------+\n",
      "|  class   |   precision    | recall |    F1 score    |\n",
      "+----------+----------------+--------+----------------+\n",
      "| positive | 0.460088981942 | 0.879  | 0.604019927847 |\n",
      "| negative | 0.661944308745 | 0.6775 | 0.669631826044 |\n",
      "| neutral  | 0.257575757576 | 0.017  | 0.031894934334 |\n",
      "| average  | 0.459869682754 | 0.5245 | 0.435182229408 |\n",
      "+----------+----------------+--------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# baseline estimate with ratio \n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "same = [0,0,0]\n",
    "truth = [0,0,0]\n",
    "predict = [0,0,0]\n",
    "X_train=np.load('X_test_sentiment_ratio.npy')\n",
    "y_train=np.load('y_test.npy')\n",
    "for i,each in enumerate(X_train):\n",
    "    if (each==1 and y_train[i]==1):\n",
    "        same[0]+=1\n",
    "    elif (each==-1 and y_train[i]==-1):\n",
    "        same[1]+=1\n",
    "    elif (each==0 and y_train[i]==0):\n",
    "        same[2]+=1\n",
    "    if y_train[i]==1:\n",
    "        truth[0] += 1\n",
    "    elif y_train[i]==-1:\n",
    "        truth[1]+=1\n",
    "    elif y_train[i]==0:\n",
    "        truth[2]+=1\n",
    "    if each==1:\n",
    "        predict[0]+=1\n",
    "    elif each==-1:\n",
    "        predict[1]+=1\n",
    "    elif each==0:\n",
    "        predict[2]+=1\n",
    "t = PrettyTable(['class', 'precision','recall','F1 score'])\n",
    "recall_sum = 0\n",
    "precision_sum = 0\n",
    "f1_sum = 0\n",
    "recall = 1.0*same[0]/truth[0]\n",
    "precision = 1.0*same[0]/predict[0]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['positive', precision,recall,f1])\n",
    "recall = 1.0*same[1]/truth[1]\n",
    "precision = 1.0*same[1]/predict[1]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['negative', precision,recall,f1])\n",
    "recall = 1.0*same[2]/truth[2]\n",
    "precision = 1.0*same[2]/predict[2]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['neutral', precision,recall,f1])\n",
    "t.add_row(['average', precision_sum/3, recall_sum/3,f1_sum/3 ])\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+----------------+\n",
      "|  class   |   precision    | recall |    F1 score    |\n",
      "+----------+----------------+--------+----------------+\n",
      "| positive | 0.418231169419 | 0.9245 | 0.575922753465 |\n",
      "| negative | 0.785214785215 | 0.393  | 0.523825391536 |\n",
      "| neutral  | 0.278546712803 | 0.0805 | 0.124903025601 |\n",
      "| average  | 0.493997555812 | 0.466  | 0.408217056868 |\n",
      "+----------+----------------+--------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# baseline estimate without ratio \n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "same = [0,0,0]\n",
    "truth = [0,0,0]\n",
    "predict = [0,0,0]\n",
    "X_train=np.load('X_test_sentiment.npy')\n",
    "y_train=np.load('y_test.npy')\n",
    "for i,each in enumerate(X_train):\n",
    "    if (each==1 and y_train[i]==1):\n",
    "        same[0]+=1\n",
    "    elif (each==-1 and y_train[i]==-1):\n",
    "        same[1]+=1\n",
    "    elif (each==0 and y_train[i]==0):\n",
    "        same[2]+=1\n",
    "    if y_train[i]==1:\n",
    "        truth[0] += 1\n",
    "    elif y_train[i]==-1:\n",
    "        truth[1]+=1\n",
    "    elif y_train[i]==0:\n",
    "        truth[2]+=1\n",
    "    if each==1:\n",
    "        predict[0]+=1\n",
    "    elif each==-1:\n",
    "        predict[1]+=1\n",
    "    elif each==0:\n",
    "        predict[2]+=1\n",
    "t = PrettyTable(['class', 'precision','recall','F1 score'])\n",
    "recall_sum = 0\n",
    "precision_sum = 0\n",
    "f1_sum = 0\n",
    "recall = 1.0*same[0]/truth[0]\n",
    "precision = 1.0*same[0]/predict[0]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['positive', precision,recall,f1])\n",
    "recall = 1.0*same[1]/truth[1]\n",
    "precision = 1.0*same[1]/predict[1]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['negative', precision,recall,f1])\n",
    "recall = 1.0*same[2]/truth[2]\n",
    "precision = 1.0*same[2]/predict[2]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['neutral', precision,recall,f1])\n",
    "t.add_row(['average', precision_sum/3, recall_sum/3,f1_sum/3 ])\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+----------------+\n",
      "|  class   |   precision    | recall |    F1 score    |\n",
      "+----------+----------------+--------+----------------+\n",
      "| positive | 0.787401574803 |  0.75  | 0.768245838668 |\n",
      "| negative | 0.734974958264 | 0.8805 | 0.80118289354  |\n",
      "| neutral  | 0.655091230135 | 0.5565 | 0.601784266018 |\n",
      "| average  | 0.725822587734 | 0.729  | 0.723737666075 |\n",
      "+----------+----------------+--------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# LSTM model result evaluation\n",
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "same = [0,0,0]\n",
    "truth = [0,0,0]\n",
    "predict = [0,0,0]\n",
    "X_train=np.load('LSTM.npy')\n",
    "def mapper(x):\n",
    "    res=np.argmax(x)\n",
    "    if res>=2:\n",
    "        return -1\n",
    "    else:\n",
    "        return res\n",
    "X_train=list(map(lambda x:mapper(x),X_train))\n",
    "y_train=np.load('y_test.npy')\n",
    "for i,each in enumerate(X_train):\n",
    "    if (each==1 and y_train[i]==1):\n",
    "        same[0]+=1\n",
    "    elif (each==-1 and y_train[i]==-1):\n",
    "        same[1]+=1\n",
    "    elif (each==0 and y_train[i]==0):\n",
    "        same[2]+=1\n",
    "    if y_train[i]==1:\n",
    "        truth[0] += 1\n",
    "    elif y_train[i]==-1:\n",
    "        truth[1]+=1\n",
    "    elif y_train[i]==0:\n",
    "        truth[2]+=1\n",
    "    if each==1:\n",
    "        predict[0]+=1\n",
    "    elif each==-1:\n",
    "        predict[1]+=1\n",
    "    elif each==0:\n",
    "        predict[2]+=1\n",
    "t = PrettyTable(['class', 'precision','recall','F1 score'])\n",
    "recall_sum = 0\n",
    "precision_sum = 0\n",
    "f1_sum = 0\n",
    "recall = 1.0*same[0]/truth[0]\n",
    "precision = 1.0*same[0]/predict[0]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['positive', precision,recall,f1])\n",
    "recall = 1.0*same[1]/truth[1]\n",
    "precision = 1.0*same[1]/predict[1]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['negative', precision,recall,f1])\n",
    "recall = 1.0*same[2]/truth[2]\n",
    "precision = 1.0*same[2]/predict[2]\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "recall_sum += recall\n",
    "precision_sum += precision\n",
    "f1_sum += f1\n",
    "t.add_row(['neutral', precision,recall,f1])\n",
    "t.add_row(['average', precision_sum/3, recall_sum/3,f1_sum/3 ])\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
